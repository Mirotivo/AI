# ğŸ¤– Offline AI Voice Assistant

A fully offline voice assistant powered by Whisper (speech-to-text), Llama 3.2 (AI), and Piper (text-to-speech). Everything runs locally on your computer with complete privacy.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- ğŸ¤ **Voice Input** - Speak naturally to your AI assistant
- ğŸ§  **Intelligent Responses** - Powered by Llama 3.2 language model
- ğŸ”Š **Natural Speech Output** - High-quality text-to-speech
- ğŸ”’ **100% Offline** - No internet required, complete privacy
- âš¡ **Pure Python Pipeline** - Optimized for speed (120-250ms faster)
- ğŸ’¬ **Text Mode** - Type your questions when in quiet environments
- ğŸŒ **Multi-language Support** - Whisper supports 90+ languages

---

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+** - [Download here](https://www.python.org/downloads/)
2. **Ollama** - [Download here](https://ollama.ai)
3. **Git** (optional) - For cloning the repository

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/mirotivo/AI.git
cd AI

# 2. Install Python dependencies
pip install -r requirements.txt

# 3. Install Ollama and download the model
# Download Ollama from https://ollama.ai
ollama pull llama3.2:3b

# 4. Set up Piper voice model (auto-downloads on first run)
# Or manually download from: https://github.com/OHF-Voice/piper1-gpl/releases

# 5. Copy and configure environment file
copy .env.example .env
# Edit .env to customize settings

# 6. Run the assistant
python voice_assistant.py
```

### First Run

```bash
python voice_assistant.py
```

The assistant will:
- âœ… Load Whisper model (one-time, ~140MB)
- âœ… Connect to Ollama
- âœ… Load Piper voice model (one-time, ~60MB)
- âœ… Ready to chat!

---

## âš™ï¸ Configuration

Edit `.env` file to customize:

```bash
# === Models ===
OLLAMA_MODEL=llama3.2:3b          # AI model (llama3.2:3b, llama3.2:1b, mistral, etc.)
WHISPER_MODEL=base                 # Speech recognition (tiny, base, small, medium, large)
PIPER_VOICE=en_US-lessac-medium   # Voice model

# === Performance ===
WHISPER_DEVICE=cpu                 # Use 'cuda' for GPU acceleration
OLLAMA_URL=http://localhost:11434 # Ollama server URL

# === Audio Settings ===
RECORD_SECONDS=5                   # Recording duration
SAMPLE_RATE=16000                  # Audio sample rate
CHANNELS=1                         # Mono audio

# === Modes ===
TEXT_MODE=false                    # false = voice input, true = keyboard input
CONTINUOUS_MODE=false              # Auto-continue without confirmation
USE_PYTHON_PIPER=true             # Use Python library (faster) vs executable

# === System ===
SYSTEM_PROMPT=You are a helpful AI assistant. Keep responses concise and natural.
LOG_LEVEL=INFO                     # DEBUG, INFO, WARNING, ERROR
```

---

## ğŸ¯ Usage

### Voice Mode (Default)

```
$ python voice_assistant.py

ğŸ¤– Voice Assistant Started
â€¢ Press Enter to start speaking
â€¢ Speak clearly for 5 seconds
â€¢ Press 'q' + Enter to quit

[Press Enter]
ğŸ¤ Listening...
You: What's the weather like today?
ğŸ¤” Thinking...
Assistant: I don't have access to real-time weather data...
ğŸ”Š Speaking...
```

### Text Mode

Set `TEXT_MODE=true` in `.env`:

```
$ python voice_assistant.py

ğŸ¤– Voice Assistant Started (Text Mode)
â€¢ Type your message and press Enter
â€¢ Type 'exit' or 'quit' to exit
â€¢ Type 'reset' to clear history

You: Hello
ğŸ¤” Thinking...
Assistant: Hello! How can I help you today?

You: _
```

---

## ğŸ“¦ Dependencies

### Core Components

| Component | Purpose | Size | Installation |
|-----------|---------|------|--------------|
| **Whisper** | Speech-to-text AI | ~140MB | `pip install openai-whisper` |
| **Ollama** | LLM server | ~2GB | [ollama.ai](https://ollama.ai) |
| **Piper** | Text-to-speech | ~60MB | Auto-downloads voice models |
| **Python 3.8+** | Runtime | ~100MB | [python.org](https://python.org) |

### Python Packages

```bash
pip install -r requirements.txt
```

Includes:
- `openai-whisper` - Speech recognition
- `sounddevice` - Audio playback
- `soundfile` - Audio file handling
- `piper-tts` - Text-to-speech (Python library)
- `requests` - API communication
- `python-dotenv` - Configuration
- `colorama` - Terminal colors
- `numpy` - Audio processing

### Optional Tools

- **FFmpeg** - Audio format conversion (auto-installed with Whisper on Windows)
- **CUDA Toolkit** - GPU acceleration (for `WHISPER_DEVICE=cuda`)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   You Speak â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Whisper AI    â”‚  Speech â†’ Text
â”‚  (Python lib)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ "What is AI?"
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚  Generate Response
â”‚  Llama 3.2 3B   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ "AI stands for..."
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Piper TTS     â”‚  Text â†’ Speech
â”‚  (Python lib)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  You Hear   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pure Python Pipeline

All components use Python libraries (no subprocess overhead):
- âœ… **Whisper**: Direct numpy array processing
- âœ… **Piper**: Python library with AudioChunk handling
- âœ… **Ollama**: Python requests library

**Performance gain**: 120-250ms faster per interaction!

---

## ğŸ“ Project Structure

```
AI/
â”œâ”€â”€ voice_assistant.py              # Main application
â”œâ”€â”€ config.py                       # Configuration loader
â”œâ”€â”€ .env                            # Your settings
â”œâ”€â”€ .env.example                    # Template
â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚
â”œâ”€â”€ modules/                        # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ speech_to_text.py          # Whisper integration (Python)
â”‚   â”œâ”€â”€ llm_client.py               # Ollama client
â”‚   â”œâ”€â”€ text_to_speech.py          # Piper (executable version)
â”‚   â””â”€â”€ text_to_speech_python.py   # Piper (Python library)
â”‚
â””â”€â”€ piper/                          # Piper TTS files
    â”œâ”€â”€ piper.exe                   # Executable (Windows)
    â””â”€â”€ voices/                     # Voice models (.onnx files)
        â””â”€â”€ en_US-lessac-medium.onnx
```

---


## ğŸ¤ Available Whisper Models

| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| `tiny` | ~40MB | Fastest | ğŸŒŸğŸŒŸ | Quick testing |
| `base` | ~140MB | Fast | ğŸŒŸğŸŒŸğŸŒŸ | Default, balanced |
| `small` | ~460MB | Medium | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ | Better accuracy |
| `medium` | ~1.5GB | Slow | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ | High accuracy |
| `large` | ~2.9GB | Slowest | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ | Best accuracy |

**Recommendation**: Start with `base` for best balance.

---

## ğŸ—£ï¸ Available Piper Voices

### English Voices

| Voice | Quality | Speed | Gender | Accent |
|-------|---------|-------|--------|--------|
| `en_US-lessac-medium` | High | Medium | Male | American |
| `en_US-amy-medium` | High | Medium | Female | American |
| `en_GB-alan-medium` | High | Medium | Male | British |
| `en_GB-jenny_dioco-medium` | High | Medium | Female | British |

[Download more voices](https://github.com/OHF-Voice/piper1-gpl/blob/main/VOICES.md)

### Adding New Voices

```bash
# 1. Download .onnx and .json files from Piper releases
# 2. Copy to piper/voices/
# 3. Update .env
PIPER_VOICE=en_GB-alan-medium
```

---

## ğŸ¤– Available Ollama Models

| Model | Size | Speed | Use Case |
|-------|------|-------|----------|
| `llama3.2:1b` | ~1.3GB | Fastest | Quick responses |
| `llama3.2:3b` | ~2GB | Fast | Default, balanced |
| `llama3.1:8b` | ~4.7GB | Medium | Better reasoning |
| `mistral:7b` | ~4.1GB | Medium | Creative writing |
| `codellama:7b` | ~3.8GB | Medium | Code assistance |

```bash
# Download a model
ollama pull llama3.2:3b

# List installed models
ollama list

# Remove a model
ollama rm model_name
```

---

## ğŸŒ Multi-language Support

Whisper supports 90+ languages. Just speak in your language!

### Example Configuration

```bash
# For Spanish
PIPER_VOICE=es_ES-carlfm-x_low
# Speak in Spanish, get Spanish responses

# For French  
PIPER_VOICE=fr_FR-siwis-medium
# Speak in French, get French responses

# For German
PIPER_VOICE=de_DE-thorsten-medium
# Speak in German, get German responses
```

[See all supported languages](https://github.com/OHF-Voice/piper1-gpl/blob/main/VOICES.md)

---

## ğŸ’¡ Tips & Best Practices

### For Best Voice Recognition
- Speak clearly and at normal pace
- Use a good quality microphone
- Reduce background noise
- Increase `RECORD_SECONDS` for longer questions

### For Better AI Responses
- Be specific in your questions
- Provide context when needed
- Use the `SYSTEM_PROMPT` to set behavior
- Use larger models for complex queries

### For Faster Performance
- Use `tiny` or `base` Whisper models
- Use smaller Ollama models (1b, 3b)
- Enable `USE_PYTHON_PIPER=true`
- Consider GPU acceleration

### For Privacy
- All data stays on your computer
- No internet connection needed
- Models run locally
- No data sent to cloud services

---

## ğŸ”’ Privacy & Security

### Data Privacy
- âœ… **100% Offline** - No internet required after setup
- âœ… **No Cloud Services** - All processing on your PC
- âœ… **No Telemetry** - No usage data collected
- âœ… **Complete Control** - You own all your data

### What Data is Stored?
- **Locally**: Conversation history (in memory, cleared on restart)
- **Not Stored**: Audio recordings (processed then discarded)
- **Models**: Downloaded once, stored locally

---

## ğŸ“Š System Requirements

### Minimum
- **CPU**: Dual-core 2.0 GHz
- **RAM**: 4GB
- **Storage**: 5GB free space
- **OS**: Windows 10+, macOS 10.15+, Linux

### Recommended
- **CPU**: Quad-core 3.0 GHz+
- **RAM**: 8GB+
- **Storage**: 10GB+ SSD
- **GPU**: NVIDIA GPU with CUDA (optional, for acceleration)

### Storage Breakdown
| Component | Size |
|-----------|------|
| Python + packages | ~500MB |
| Whisper base model | ~140MB |
| Ollama + Llama 3.2 | ~2GB |
| Piper + voice | ~80MB |
| **Total** | **~3GB** |

---

## ğŸ› ï¸ Development

### Running Tests

```bash
# Test microphone
python -c "from modules.speech_to_text import SpeechToText; stt = SpeechToText(); stt.record_audio()"

# Test Ollama
python -c "from modules.llm_client import LLMClient; client = LLMClient(); print(client.generate('Hello'))"

# Test TTS
python -c "from modules.text_to_speech_python import TextToSpeech; tts = TextToSpeech(voice_model_path='piper/voices/en_US-lessac-medium.onnx'); tts.speak('Hello')"
```

### Code Structure

```python
# Main application
voice_assistant.py
  â”œâ”€â”€ VoiceAssistant.__init__()      # Initialize components
  â”œâ”€â”€ VoiceAssistant.run()            # Main loop
  â”œâ”€â”€ process_voice_input()           # Handle voice mode
  â””â”€â”€ process_text_input()            # Handle text mode

# Configuration
config.py
  â””â”€â”€ Config class                    # Load settings from .env

# Modules
modules/
  â”œâ”€â”€ speech_to_text.py               # Whisper wrapper
  â”‚   â”œâ”€â”€ record_audio()              # Capture audio
  â”‚   â””â”€â”€ transcribe()                # Speech â†’ text
  â”‚
  â”œâ”€â”€ llm_client.py                   # Ollama client
  â”‚   â”œâ”€â”€ generate()                  # Get AI response
  â”‚   â””â”€â”€ reset_conversation()        # Clear history
  â”‚
  â””â”€â”€ text_to_speech_python.py       # Piper wrapper
      â”œâ”€â”€ synthesize()                # Text â†’ audio
      â””â”€â”€ speak()                     # Generate + play
```

---

## ğŸš§ Known Limitations

- **Response Time**: 2-5 seconds per interaction (depends on model size)
- **Model Size**: Larger models require more RAM/storage
- **Language Mixing**: Piper voices are language-specific
- **Real-time Data**: No access to current events/weather
- **Complex Reasoning**: Limited by model capabilities

---

## ğŸ—ºï¸ Roadmap

- [ ] Add wake word detection ("Hey Assistant")
- [ ] Support for streaming responses
- [ ] Multi-turn conversation improvements
- [ ] Custom voice training
- [ ] Web interface
- [ ] Mobile app version
- [ ] Plugin system for extensions

---

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### How to Contribute

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/mirotivo/AI/issues)
- **Discussions**: [GitHub Discussions](https://github.com/mirotivo/AI/discussions)

---

## ğŸ™ Acknowledgments

This project stands on the shoulders of giants. Special thanks to:

### Core Technologies

- **[OpenAI Whisper](https://github.com/openai/whisper)** - Open-source speech recognition
  - GitHub: https://github.com/openai/whisper
  - PyPI: `pip install openai-whisper`
  - License: MIT

- **[Ollama](https://github.com/ollama/ollama)** - Run large language models locally
  - GitHub: https://github.com/ollama/ollama
  - Website: https://ollama.ai
  - License: MIT

- **[Piper TTS](https://github.com/OHF-Voice/piper1-gpl)** - Fast, local text-to-speech
  - GitHub: https://github.com/OHF-Voice/piper1-gpl
  - PyPI: `pip install piper-tts`
  - License: GPL

- **[Meta Llama](https://llama.meta.com/)** - Open-source language models
  - Website: https://llama.meta.com/
  - Models: llama3.2:1b, llama3.2:3b, llama3.1:8b

### Why These Projects?

All chosen for being:
- âœ… **Open Source** - Free to use and modify
- âœ… **Offline Capable** - No internet required
- âœ… **High Quality** - Production-ready
- âœ… **Well Maintained** - Active development
- âœ… **Python Compatible** - Easy integration

**Note**: While we use Python packages from PyPI (`pip install`), the source code lives on GitHub. Check the links above to explore, contribute, or learn more!

---

## ğŸ“š Additional Resources

- [Whisper Documentation](https://github.com/openai/whisper)
- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Piper Documentation](https://github.com/OHF-Voice/piper1-gpl)
- [Python Speech Processing](https://realpython.com/python-speech-recognition/)

---

**Made with â¤ï¸ for offline AI enthusiasts**

*Last updated: December 2025*
